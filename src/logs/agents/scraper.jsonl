{"agent": "scrape_agent", "event": "agent_step", "time": "2024-07-21 10:32:27.111323+05:30", "tool": "Read website content", "tool_input": "{\"website_url\": \"https://huggingface.co/microsoft/Phi-3-vision-128k-instruct\"}", "log": "Thought: I need to scrape the content from the provided URL using the `Read website content` tool.\n\nAction: Read website content\nAction Input: {\"website_url\": \"https://huggingface.co/microsoft/Phi-3-vision-128k-instruct\"}"}
{"agent": "scrape_agent", "event": "agent_step", "time": "2024-07-21 10:32:28.686462+05:30", "tool": "Read website content", "tool_input": "{\"website_url\": \"https://huggingface.co/microsoft/Phi-3-vision-128k-instruct\"}", "log": "Thought: I have scraped the content from the provided URL.\n\nAction: Read website content\nAction Input: {\"website_url\": \"https://huggingface.co/microsoft/Phi-3-vision-128k-instruct\"}"}
{"agent": "scrape_agent", "event": "agent_finish", "time": "2024-07-21 10:33:11.682221+05:30", "tool": "Unknown", "tool_input": "Unknown", "log": "Thought: I now know the final answer.\n\nFinal Answer: The scraped text from the URL is:\n\nmicrosoft/Phi-3-vision-128k-instruct \u00b7 Hugging Face\nHugging Face\n\t\t\t\t\tModels\n\t\t\t\t\tDatasets\n\t\t\t\t\tSpaces\n\t\t\t\t\tPosts\n\t\t\t\t\tDocs\n\t\t\tSolutions\nPricing\nLog In\nSign Up\nmicrosoft\n/\nPhi-3-vision-128k-instruct\nlike\n813\nText Generation\nTransformers\nSafetensors\nmultilingual\nphi3_v\nnlp\ncode\nvision\nconversational\ncustom_code\nLicense:\nmit\n\t\t\tModel card\nFiles\nFiles and versions\n\t\t\tCommunity\n\t\t\t54\n\t\t\tTrain\n\t\t\tUse this model\n\t\t\t\t\t\tEdit model card\nModel Summary\nIntended Uses\nHow to Use\nChat Format\nSample inference code\nHow to finetune?\nResponsible AI Considerations\nTraining\nModel\nDatasets\nBenchmarks\nSoftware\nHardware\nLicense\nTrademarks\n\t\tModel Summary\nThe Phi-3-Vision-128K-Instruct is a lightweight, state-of-the-art open multimodal model built upon datasets which include - synthetic data and filtered publicly available websites - with a focus on very high-quality, reasoning dense data both on text and vision. The model belongs to the Phi-3 model family, and the multimodal version comes with 128K context length (in tokens) it can support. The model underwent a rigorous enhancement process, incorporating both supervised fine-tuning and direct preference optimization to ensure precise instruction adherence and robust safety measures.\nResources and Technical Documentation:\nPhi-3 Microsoft Blog\nPhi-3 Technical Report\nPhi-3 on Azure AI Studio\nPhi-3 Cookbook\nShort Context\nLong Context\nMini\n4K [HF] ; [ONNX] ; [GGUF]\n128K [HF] ; [ONNX]\nSmall\n8K [HF] ; [ONNX]\n128K [HF] ; [ONNX]\nMedium\n4K [HF] ; [ONNX]\n128K [HF] ; [ONNX]\nVision\n128K [HF] ; [ONNX]\n\t\tIntended Uses\nPrimary use cases\nThe model is intended for broad commercial and research use in English. The model provides uses for general purpose AI systems and applications with visual and text input capabilities which require \nmemory/compute constrained environments;\nlatency bound scenarios;\ngeneral image understanding;\nOCR;\nchart and table understanding.\nOur model is designed to accelerate research on efficient language and multimodal models, for use as a building block for generative AI powered features.\nUse case considerations\nOur models are not specifically designed or evaluated for all downstream purposes. Developers should consider common limitations of language models as they select use cases, and evaluate and mitigate for accuracy, safety, and fairness before using within a specific downstream use case, particularly for high-risk scenarios. \nDevelopers should be aware of and adhere to applicable laws or regulations (including privacy, trade compliance laws, etc.) that are relevant to their use case. \nNothing contained in this Model Card should be interpreted as or deemed a restriction or modification to the license the model is released under.\n\t\tHow to Use\nPhi-3-Vision-128K-Instruct has been integrated in the development version (4.40.2) of transformers. Until the official version is released through pip, ensure that you are doing one of the following:\nWhen loading the model, ensure that trust_remote_code=True is passed as an argument of the from_pretrained() function.\nUpdate your local transformers to the development version: pip uninstall -y transformers && pip install git+https://github.com/huggingface/transformers. The previous command is an alternative to cloning and installing from the source.\nThe current transformers version can be verified with: pip list | grep transformers.\nExamples of required packages:\nflash_attn==2.5.8\nnumpy==1.24.4\nPillow==10.3.0\nRequests==2.31.0\ntorch==2.3.0\ntorchvision==0.18.0\ntransformers==4.40.2\nPhi-3-Vision-128K-Instruct is also available in Azure AI Studio.\n\t\tChat Format\nGiven the nature of the training data, the Phi-3-Vision-128K-Instruct model is best suited for a single image input wih prompts using the chat format as follows. \nYou can provide the prompt as a single image with a generic template as follow:\n<|user|>\\n<|image_1|>\\n{prompt}<|end|>\\n<|assistant|>\\n \nwhere the model generates the text after <|assistant|> . In case of multi-turn conversation, the prompt can be formatted as follows:\n<|user|>\\n<|image_1|>\\n{prompt_1}<|end|>\\n<|assistant|>\\n{response_1}<|end|>\\n<|user|>\\n{prompt_2}<|end|>\\n<|assistant|>\\n \n\t\tSample inference code\nThis code snippets show how to get quickly started with running the model on a GPU:\nfrom PIL import Image \nimport requests \nfrom transformers import AutoModelForCausalLM \nfrom transformers import AutoProcessor \nmodel_id = \"microsoft/Phi-3-vision-128k-instruct\" \nmodel = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"cuda\", trust_remote_code=True, torch_dtype=\"auto\", _attn_implementation='flash_attention_2') # use _attn_implementation='eager' to disable flash attention\nprocessor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True) \nmessages = [ {\"role\": \"user\", \"content\": \"<|image_1|>\\nWhat is shown in this image?\"}, {\"role\": \"assistant\", \"content\": \"The chart displays the percentage of respondents who agree with various statements about their preparedness for meetings. It shows five categories: 'Having clear and pre-defined goals for meetings', 'Knowing where to find the information I need for a meeting', 'Understanding my exact role and responsibilities when I'm invited', 'Having tools to manage admin tasks like note-taking or summarization', and 'Having more focus time to sufficiently prepare for meetings'. Each category has an associated bar indicating the level of agreement, measured on a scale from 0% to 100%.\"}, {\"role\": \"user\", \"content\": \"Provide insightful questions to spark discussion.\"} \n] \nurl = \"https://assets-c4akfrf5b4d3f4b7.z01.azurefd.net/assets/2024/04/BMDataViz_661fb89f3845e.png\" \nimage = Image.open(requests.get(url, stream=True).raw) \nprompt = processor.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\ninputs = processor(prompt, [image], return_tensors=\"pt\").to(\"cuda:0\") \ngeneration_args = { \"max_new_tokens\": 500, \"temperature\": 0.0, \"do_sample\": False, \n} \ngenerate_ids = model.generate(**inputs, eos_token_id=processor.tokenizer.eos_token_id, **generation_args) \n# remove input tokens \ngenerate_ids = generate_ids[:, inputs['input_ids'].shape[1]:]\nresponse = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0] \nprint(response) \nAdditional basic examples are provided here.\n\t\tHow to finetune?\nWe recommend user to take a look at the Phi-3 CookBook finetuning recipe for Vision\n\t\tResponsible AI Considerations\nLike other models, the Phi family of models can potentially behave in ways that are unfair, unreliable, or offensive. Some of the limiting behaviors to be aware of include: \nQuality of Service: The Phi models are trained primarily on English text. Languages other than English will experience worse performance. English language varieties with less representation in the training data might experience worse performance than standard American English. \nRepresentation of Harms & Perpetuation of Stereotypes: These models can over- or under-represent groups of people, erase representation of some groups, or reinforce demeaning or negative stereotypes. Despite safety post-training, these limitations may still be present due to differing levels of representation of different groups or prevalence of examples of negative stereotypes in training data that reflect real-world patterns and societal biases. \nInappropriate or Offensive Content: These models may produce other types of inappropriate or offensive content, which may make it inappropriate to deploy for sensitive contexts without additional mitigations that are specific to the use case. \nInformation Reliability: Language models can generate nonsensical content or fabricate content that might sound reasonable but is inaccurate or outdated. \nLimited Scope for Code: Majority of Phi-3 training data is based in Python and use common packages such as \"typing, math, random, collections, datetime, itertools\". If the model generates Python scripts that utilize other packages or scripts in other languages, we strongly recommend users manually verify all API uses.\nDevelopers should apply responsible AI best practices and are responsible for ensuring that a specific use case complies with relevant laws and regulations (e.g. privacy, trade, etc.). Important areas for consideration include: \nAllocation: Models may not be suitable for scenarios that could have consequential impact on legal status or the allocation of resources or life opportunities (ex: housing, employment, credit, etc.) without further assessments and additional debiasing techniques.\nHigh-Risk Scenarios: Developers should assess suitability of using models in high-risk scenarios where unfair, unreliable or offensive outputs might be extremely costly or lead to harm. This includes providing advice in sensitive or expert domains where accuracy and reliability are critical (ex: legal or health advice). Additional safeguards should be implemented at the application level according to the deployment context.\nMisinformation: Models may produce inaccurate information. Developers should follow transparency best practices and inform end-users they are interacting with an AI system. At the application level, developers can build feedback mechanisms and pipelines to ground responses in use-case specific, contextual information, a technique known as Retrieval Augmented Generation (RAG).\nGeneration of Harmful Content: Developers should assess outputs for their context and use available safety classifiers or custom solutions appropriate for their use case.\nMisuse: Other forms of misuse such as fraud, spam, or malware production may be possible, and developers should ensure that their applications do not violate applicable laws and regulations.\nIdentification of individuals: models with vision capabilities may have the potential to uniquely identify individuals in images. Safety post-training steers the model to refuse such requests, but developers should consider and implement, as appropriate, additional mitigations or user consent flows as required in their respective jurisdiction, (e.g., building measures to blur faces in image inputs before processing.\n\t\tTraining\n\t\tModel\nArchitecture: Phi-3-Vision-128K-Instruct has 4.2B parameters and contains image encoder, connector, projector, and Phi-3 Mini language model.\nInputs: Text and Image. It\u2019s best suited for prompts using the chat format. \nContext length: 128K tokens\nGPUs: 512 H100-80G\nTraining time: 1.5 days\nTraining data: 500B vision and text tokens\nOutputs: Generated text in response to the input\nDates: Our models were trained between February and April 2024\nStatus: This is a static model trained on an offline text dataset with cutoff date Mar 15, 2024. Future versions of the tuned models may be released as we improve models.\nRelease Type: Open weight release\nRelease dates: The model weight is released on May 21, 2024.\n\t\tDatasets\nOur training data includes a wide variety of sources, and is a combination of \npublicly available documents filtered rigorously for quality, selected high-quality educational data and code;\nselected high-quality image-text interleave;\nnewly created synthetic, \u201ctextbook-like\u201d data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (science, daily activities, theory of mind, etc.), newly created image data, e.g., chart/table/diagram/slides;\nhigh quality chat format supervised data covering various topics to reflect human preferences on different aspects such as instruct-following, truthfulness, honesty and helpfulness.\nThe data collection process involved sourcing information from publicly available documents, with a meticulous approach to filtering out undesirable documents and images. To safeguard privacy, we carefully filtered various image and text data sources to remove or scrub any potentially personal data from the training data.\nMore details can be found in the Phi-3 Technical Report.\n\t\tBenchmarks\nTo understand the capabilities, we compare Phi-3-Vision-128K-Instruct with a set of models over a variety of zero-shot benchmarks using our internal benchmark platform.\nBenchmark\nPhi-3 Vision-128K-In\nLlaVA-1.6 Vicuna-7B\nQWEN-VL Chat\nLlama3-Llava-Next-8B\nClaude-3 Haiku\nGemini 1.0 Pro V\nGPT-4V-Turbo\nMMMU\n40.4\n34.2\n39.0\n36.4\n40.7\n42.0\n55.5\nMMBench\n80.5\n76.3\n75.8\n79.4\n62.4\n80.0\n86.1\nScienceQA\n90.8\n70.6\n67.2\n73.7\n72.0\n79.7\n75.7\nMathVista\n44.5\n31.5\n29.4\n34.8\n33.2\n35.0\n47.5\nInterGPS\n38.1\n20.5\n22.3\n24.6\n32.1\n28.6\n41.0\nAI2D\n76.7\n63.1\n59.8\n66.9\n60.3\n62.8\n74.7\nChartQA\n81.4\n55.0\n50.9\n65.8\n59.3\n58.0\n62.3\nTextVQA\n70.9\n64.6\n59.4\n55.7\n62.7\n64.7\n68.1\nPOPE\n85.8\n87.2\n82.6\n87.0\n74.4\n84.2\n83.7\n\t\tSoftware\nPyTorch\nTransformers\nFlash-Attention\n\t\tHardware\nNote that by default, the Phi-3-Vision-128K model uses flash attention, which requires certain types of GPU hardware to run. We have tested on the following GPU types:\nNVIDIA A100\nNVIDIA A6000\nNVIDIA H100\n\t\tLicense\nThe model is licensed under the MIT license.\n\t\tTrademarks\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow\u202fMicrosoft\u2019s Trademark & Brand Guidelines. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party\u2019s policies.\nDownloads last month196,820\nSafetensors\nModel size\n4.15B params\nTensor type\nBF16\n\t\t\t\t\t\t\u00b7\nText Generation\nInference API (serverless) does not yet support model repos that contain custom code.\n\t\t\t\t\t\tSpaces using\n\t\t\t\t\t\tmicrosoft/Phi-3-vision-128k-instruct\n25\n\ud83d\ude3b\nysharma/Microsoft_Phi-3-Vision-128k\ud83d\ude3b\nprithivMLmods/PHI3-VISION\ud83d\udcda\naifeifei798/nemotron-4-340b-instruct\ud83c\udfc3\nsimonraj/PersonalTrainer-Arnold\ud83d\udcda\naifeifei798/Mistral-NeMo-12B-Instruct\ud83c\udf10\ndwb2023/hf_extractor\ud83d\ude3b\nTypeGPT/Microsoft_Phi-3-Vision-128k\ud83d\udcc8\nbaconnier/BANKER_Vision\ud83d\ude80\ndwb2023/model_explorer4\ud83d\udd2c\ud83e\uddeb\ndwb2023/omniscience\ud83d\udd25\nMaziyarPanahi/Phi-3-vision-128k\ud83d\ude80\ndwb2023/model_explorer2\ud83d\ude3b\nawacke1/ImageChatbot-Phi-3-Vision-128k\ud83c\udf16\nSharryOG/AI\ud83d\udd25\nhf1024/Phi-3-vision-128k\ud83d\ude3b\nnikhilalgorithma/vision1\ud83d\ude3b\naggaranu/Microsoft_Phi-3-Vision-128k\ud83d\ude3b\nzohadev/Microsoft-Phi-3-Vision-128k\ud83d\udd25\nShankarm08/phi3vision\ud83d\ude3b\nDorjzodovsuren/Microsoft_Phi-3-Vision-128k\n+ 20 Spaces\n+ 5 Spaces\n\t\t\t\t\t\tCollection including\n\t\t\t\t\t\tmicrosoft/Phi-3-vision-128k-instruct\nPhi-3\n\t\t\t\t\tCollection\nPhi-3 family of small language and multi-modal models. Language models are available in short- and long-context lengths.\n\u2022 \n23 items\n\u2022 \nUpdated\n\t\t\t\t\t10 days ago\n\u2022\n\t\t\t\t\t372\nCompany\n\u00a9 Hugging Face\nTOS\nPrivacy\nAbout\nJobs\nWebsite\nModels\nDatasets\nSpaces\nPricing\nDocs"}
{"agent": "scrape_agent", "event": "agent_step", "time": "2024-07-21 11:44:38.345322+05:30", "tool": "Read website content", "tool_input": "{\"website_url\": \"https://huggingface.co/microsoft/Phi-3-vision-128k-instruct\"}", "log": "Thought: I need to scrape the content from the provided URL using the `Read website content` tool.\n\nAction: Read website content\nAction Input: {\"website_url\": \"https://huggingface.co/microsoft/Phi-3-vision-128k-instruct\"}"}
{"agent": "scrape_agent", "event": "agent_finish", "time": "2024-07-21 11:44:51.850080+05:30", "tool": "Unknown", "tool_input": "Unknown", "log": "Thought: I have scraped the content from the provided URL.\n\nFinal Answer: The scraped text from the URL is:\n\n\"microsoft/Phi-3-vision-128k-instruct \u00b7 Hugging Face\nHugging Face\n\t\t\t\t\tModels\n\t\t\t\t\tDatasets\n\t\t\t\t\tSpaces\n\t\t\t\t\tPosts\n\t\t\t\t\tDocs\n\t\t\tSolutions\nPricing\nLog In\nSign Up\nmicrosoft\n/\nPhi-3-vision-128k-instruct\nlike\n813\nText Generation\nTransformers\nSafetensors\nmultilingual\nphi3_v\nnlp\ncode\nvision\nconversational\ncustom_code\nLicense:\nmit\n\t\t\tModel card\nFiles\nFiles and versions\n\t\t\tCommunity\n\t\t\t54\n\t\t\tTrain\n\t\t\tUse this model\n\t\t\t\t\t\tEdit model card\nModel Summary\nIntended Uses\nHow to Use\nChat Format\nSample inference code\nHow to finetune?\nResponsible AI Considerations\nTraining\nModel\nDatasets\nBenchmarks\nSoftware\nHardware\nLicense\nTrademarks\n\t\tModel Summary\nThe Phi-3-Vision-128K-Instruct is a lightweight, state-of-the-art open multimodal model built upon datasets which include - synthetic data and filtered publicly available websites - with a focus on very high-quality, reasoning dense data both on text and vision. The model belongs to the Phi-3 model family, and the multimodal version comes with 128K context length (in tokens) it can support. The model underwent a rigorous enhancement process, incorporating both supervised fine-tuning and direct preference optimization to ensure precise instruction adherence and robust safety measures.\nResources and Technical Documentation:\nPhi-3 Microsoft Blog\nPhi-3 Technical Report\nPhi-3 on Azure AI Studio\nPhi-3 Cookbook\nShort Context\nLong Context\nMini\n4K [HF] ; [ONNX] ; [GGUF]\n128K [HF] ; [ONNX]\nSmall\n8K [HF] ; [ONNX]\n128K [HF] ; [ONNX]\nMedium\n4K [HF] ; [ONNX]\n128K [HF] ; [ONNX]\nVision\n128K [HF] ; [ONNX]\n\t\tIntended Uses\nPrimary use cases\nThe model is intended for broad commercial and research use in English. The model provides uses for general purpose AI systems and applications with visual and text input capabilities which require \nmemory/compute constrained environments;\nlatency bound scenarios;\ngeneral image understanding;\nOCR;\nchart and table understanding.\nOur model is designed to accelerate research on efficient language and multimodal models, for use as a building block for generative AI powered features.\nUse case considerations\nOur models are not specifically designed or evaluated for all downstream purposes. Developers should consider common limitations of language models as they select use cases, and evaluate and mitigate for accuracy, safety, and fairness before using within a specific downstream use case, particularly for high-risk scenarios. \nDevelopers should be aware of and adhere to applicable laws or regulations (including privacy, trade compliance laws, etc.) that are relevant to their use case. \nNothing contained in this Model Card should be interpreted as or deemed a restriction or modification to the license the model is released under.\n\t\tHow to Use\nPhi-3-Vision-128K-Instruct has been integrated in the development version (4.40.2) of transformers. Until the official version is released through pip, ensure that you are doing one of the following:\nWhen loading the model, ensure that trust_remote_code=True is passed as an argument of the from_pretrained() function.\nUpdate your local transformers to the development version: pip uninstall -y transformers && pip install git+https://github.com/huggingface/transformers. The previous command is an alternative to cloning and installing from the source.\nThe current transformers version can be verified with: pip list | grep transformers.\nExamples of required packages:\nflash_attn==2.5.8\nnumpy==1.24.4\nPillow==10.3.0\nRequests==2.31.0\ntorch==2.3.0\ntorchvision==0.18.0\ntransformers==4.40.2\nPhi-3-Vision-128K-Instruct is also available in Azure AI Studio.\n\t\tChat Format\nGiven the nature of the training data, the Phi-3-Vision-128K-Instruct model is best suited for a single image input wih prompts using the chat format as follows. \nYou can provide the prompt as a single image with a generic template as follow:\n<|user|>\\n<|image_1|>\\n{prompt}<|end|>\\n<|assistant|>\\n \nwhere the model generates the text after <|assistant|> . In case of multi-turn conversation, the prompt can be formatted as follows:\n<|user|>\\n<|image_1|>\\n{prompt_1}<|end|>\\n<|assistant|>\\n{response_1}<|end|>\\n<|user|>\\n{prompt_2}<|end|>\\n<|assistant|>\\n \n\t\tSample inference code\nThis code snippets show how to get quickly started with running the model on a GPU:\nfrom PIL import Image \nimport requests \nfrom transformers import AutoModelForCausalLM \nfrom transformers import AutoProcessor \nmodel_id = \"microsoft/Phi-3-vision-128k-instruct\" \nmodel = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"cuda\", trust_remote_code=True, torch_dtype=\"auto\", _attn_implementation='flash_attention_2') # use _attn_implementation='eager' to disable flash attention\nprocessor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True) \nmessages = [ {\"role\": \"user\", \"content\": \"<|image_1|>\\nWhat is shown in this image?\"}, {\"role\": \"assistant\", \"content\": \"The chart displays the percentage of respondents who agree with various statements about their preparedness for meetings. It shows five categories: 'Having clear and pre-defined goals for meetings', 'Knowing where to find the information I need for a meeting', 'Understanding my exact role and responsibilities when I'm invited', 'Having tools to manage admin tasks like note-taking or summarization', and 'Having more focus time to sufficiently prepare for meetings'. Each category has an associated bar indicating the level of agreement, measured on a scale from 0% to 100%.\"}, {\"role\": \"user\", \"content\": \"Provide insightful questions to spark discussion.\"} \n] \nurl = \"https://assets-c4akfrf5b4d3f4b7.z01.azurefd.net/assets/2024/04/BMDataViz_661fb89f3845e.png\" \nimage = Image.open(requests.get(url, stream=True).raw) \nprompt = processor.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\ninputs = processor(prompt, [image], return_tensors=\"pt\").to(\"cuda:0\") \ngeneration_args = { \"max_new_tokens\": 500, \"temperature\": 0.0, \"do_sample\": False, \n} \ngenerate_ids = model.generate(**inputs, eos_token_id=processor.tokenizer.eos_token_id, **generation_args) \n# remove input tokens \ngenerate_ids = generate_ids[:, inputs['input_ids'].shape[1]:]\nresponse = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0] \nprint(response) \nAdditional basic examples are provided here.\n\t\tHow to finetune?\nWe recommend user to take a look at the Phi-3 CookBook finetuning recipe for Vision\n\t\tResponsible AI Considerations\nLike other models, the Phi family of models can potentially behave in ways that are unfair, unreliable, or offensive. Some of the limiting behaviors to be aware of include: \nQuality of Service: The Phi models are trained primarily on English text. Languages other than English will experience worse performance. English language varieties with less representation in the training data might experience worse performance than standard American English. \nRepresentation of Harms & Perpetuation of Stereotypes: These models can over- or under-represent groups of people, erase representation of some groups, or reinforce demeaning or negative stereotypes. Despite safety post-training, these limitations may still be present due to differing levels of representation of different groups or prevalence of examples of negative stereotypes in training data that reflect real-world patterns and societal biases. \nInappropriate or Offensive Content: These models may produce other types of inappropriate or offensive content, which may make it inappropriate to deploy for sensitive contexts without additional mitigations that are specific to the use case. \nInformation Reliability: Language models can generate nonsensical content or fabricate content that might sound reasonable but is inaccurate or outdated. \nLimited Scope for Code: Majority of Phi-3 training data is based in Python and use common packages such as \"typing, math, random, collections, datetime, itertools\". If the model generates Python scripts that utilize other packages or scripts in other languages, we strongly recommend users manually verify all API uses.\nDevelopers should apply responsible AI best practices and are responsible for ensuring that a specific use case complies with relevant laws and regulations (e.g. privacy, trade, etc.). Important areas for consideration include: \nAllocation: Models may not be suitable for scenarios that could have consequential impact on legal status or the allocation of resources or life opportunities (ex: housing, employment, credit, etc.) without further assessments and additional debiasing techniques.\nHigh-Risk Scenarios: Developers should assess suitability of using models in high-risk scenarios where unfair, unreliable or offensive outputs might be extremely costly or lead to harm. This includes providing advice in sensitive or expert domains where accuracy and reliability are critical (ex: legal or health advice). Additional safeguards should be implemented at the application level according to the deployment context.\nMisinformation: Models may produce inaccurate information. Developers should follow transparency best practices and inform end-users they are interacting with an AI system. At the application level, developers can build feedback mechanisms and pipelines to ground responses in use-case specific, contextual information, a technique known as Retrieval Augmented Generation (RAG).\nGeneration of Harmful Content: Developers should assess outputs for their context and use available safety classifiers or custom solutions appropriate for their use case.\nMisuse: Other forms of misuse such as fraud, spam, or malware production may be possible, and developers should ensure that their applications do not violate applicable laws and regulations.\nIdentification of individuals: models with vision capabilities may have the potential to uniquely identify individuals in images. Safety post-training steers the model to refuse such requests, but developers should consider and implement, as appropriate, additional mitigations or user consent flows as required in their respective jurisdiction, (e.g., building measures to blur faces in image inputs before processing.\n\t\tTraining\n\t\tModel\nArchitecture: Phi-3-Vision-128K-Instruct has 4.2B parameters and contains image encoder, connector, projector, and Phi-3 Mini language model.\nInputs: Text and Image. It\u2019s best suited for prompts using the chat format. \nContext length: 128K tokens\nGPUs: 512 H100-80G\nTraining time: 1.5 days\nTraining data: 500B vision and text tokens\nOutputs: Generated text in response to the input\nDates: Our models were trained between February and April 2024\nStatus: This is a static model trained on an offline text dataset with cutoff date Mar 15, 2024. Future versions of the tuned models may be released as we improve models.\nRelease Type: Open weight release\nRelease dates: The model weight is released on May 21, 2024.\n\t\tDatasets\nOur training data includes a wide variety of sources, and is a combination of \npublicly available documents filtered rigorously for quality, selected high-quality educational data and code;\nselected high-quality image-text interleave;\nnewly created synthetic, \u201ctextbook-like\u201d data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (science, daily activities, theory of mind, etc.), newly created image data, e.g., chart/table/diagram/slides;\nhigh quality chat format supervised data covering various topics to reflect human preferences on different aspects such as instruct-following, truthfulness, honesty and helpfulness.\nThe data collection process involved sourcing information from publicly available documents, with a meticulous approach to filtering out undesirable documents and images. To safeguard privacy, we carefully filtered various image and text data sources to remove or scrub any potentially personal data from the training data.\nMore details can be found in the Phi-3 Technical Report.\n\t\tBenchmarks\nTo understand the capabilities, we compare Phi-3-Vision-128K-Instruct with a set of models over a variety of zero-shot benchmarks using our internal benchmark platform.\nBenchmark\nPhi-3 Vision-128K-In\nLlaVA-1.6 Vicuna-7B\nQWEN-VL Chat\nLlama3-Llava-Next-8B\nClaude-3 Haiku\nGemini 1.0 Pro V\nGPT-4V-Turbo\nMMMU\n40.4\n34.2\n39.0\n36.4\n40.7\n42.0\n55.5\nMMBench\n80.5\n76.3\n75.8\n79.4\n62.4\n80.0\n86.1\nScienceQA\n90.8\n70.6\n67.2\n73.7\n72.0\n79.7\n75.7\nMathVista\n44.5\n31.5\n29.4\n34.8\n33.2\n35.0\n47.5\nInterGPS\n38.1\n20.5\n22.3\n24.6\n32.1\n28.6\n41.0\nAI2D\n76.7\n63.1\n59.8\n66.9\n60.3\n62.8\n74.7\nChartQA\n81.4\n55.0\n50.9\n65.8\n59.3\n58.0\n62.3\nTextVQA\n70.9\n64.6\n59.4\n55.7\n62.7\n64.7\n68.1\nPOPE\n85.8\n87.2\n82.6\n87.0\n74.4\n84.2\n83.7\n\t\tSoftware\nPyTorch\nTransformers\nFlash-Attention\n\t\tHardware\nNote that by default, the Phi-3-Vision-128K model uses flash attention, which requires certain types of GPU hardware to run. We have tested on the following GPU types:\nNVIDIA A100\nNVIDIA A6000\nNVIDIA H100\n\t\tLicense\nThe model is licensed under the MIT license.\n\t\tTrademarks\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow\u202fMicrosoft\u2019s Trademark & Brand Guidelines. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party\u2019s policies.\nDownloads last month196,820\nSafetensors\nModel size\n4.15B params\nTensor type\nBF16\n\t\t\t\t\t\t\u00b7\nText Generation\nInference API (serverless) does not yet support model repos that contain custom code.\n\t\t\t\t\t\tSpaces using\n\t\t\t\t\t\tmicrosoft/Phi-3-vision-128k-instruct\n25\n\ud83d\ude3b\nysharma/Microsoft_Phi-3-Vision-128k\ud83d\ude3b\nprithivMLmods/PHI3-VISION\ud83d\udcda\naifeifei798/nemotron-4-340b-instruct\ud83c\udfc3\nsimonraj/PersonalTrainer-Arnold\ud83d\udcda\naifeifei798/Mistral-NeMo-12B-Instruct\ud83c\udf10\ndwb2023/hf_extractor\ud83d\ude3b\nTypeGPT/Microsoft_Phi-3-Vision-128k\ud83d\udcc8\nbaconnier/BANKER_Vision\ud83d\ude80\ndwb2023/model_explorer4\ud83d\udd2c\ud83e\uddeb\ndwb2023/omniscience\ud83d\udd25\nMaziyarPanahi/Phi-3-vision-128k\ud83d\ude80\ndwb2023/model_explorer2\ud83d\ude3b\nawacke1/ImageChatbot-Phi-3-Vision-128k\ud83c\udf16\nSharryOG/AI\ud83d\udd25\nhf1024/Phi-3-vision-128k\ud83d\ude3b\nnikhilalgorithma/vision1\ud83d\ude3b\naggaranu/Microsoft_Phi-3-Vision-128k\ud83d\ude3b\nzohadev/Microsoft-Phi-3-Vision-128k\ud83d\udd25\nShankarm08/phi3vision\ud83d\ude3b\nDorjzodovsuren/Microsoft_Phi-3-Vision-128k\n+ 20 Spaces\n+ 5 Spaces\n\t\t\t\t\t\tCollection including\n\t\t\t\t\t\tmicrosoft/Phi-3-vision-128k-instruct\nPhi-3\n\t\t\t\t\tCollection\nPhi-3 family of small language and multi-modal models. Language models are available in short- and long-context lengths.\n\u2022 \n23 items\n\u2022 \nUpdated\n\t\t\t\t\t10 days ago\n\u2022\n\t\t\t\t\t372\nCompany\n\u00a9 Hugging Face\nTOS\nPrivacy\nAbout\nJobs\nWebsite\nModels\nDatasets\nSpaces\nPricing\nDocs\n\""}
{"agent": "scrape_agent", "event": "agent_step", "time": "2024-07-21 17:49:31.165059+05:30", "tool": "Read website content", "tool_input": "{\"website_url\": \"https://huggingface.co/microsoft/Phi-3-vision-128k-instruct\"}", "log": "Thought: I need to scrape the content from the provided URL.\n\nAction: Read website content\nAction Input: {\"website_url\": \"https://huggingface.co/microsoft/Phi-3-vision-128k-instruct\"}"}
{"agent": "scrape_agent", "event": "agent_step", "time": "2024-07-21 17:49:32.778249+05:30", "tool": "Read website content", "tool_input": "{\"website_url\": \"https://huggingface.co/microsoft/Phi-3-vision-128k-instruct\"}", "log": "Thought: I have scraped the content from the provided URL.\n\nAction: Read website content\nAction Input: {\"website_url\": \"https://huggingface.co/microsoft/Phi-3-vision-128k-instruct\"}"}
{"agent": "scrape_agent", "event": "agent_finish", "time": "2024-07-21 17:50:27.060280+05:30", "tool": "Unknown", "tool_input": "Unknown", "log": "Thought: I now know the final answer.\n\nFinal Answer: The scraped text from the provided URL is:\n\n\"microsoft/Phi-3-vision-128k-instruct \u00b7 Hugging Face\nHugging Face\n\t\t\t\t\tModels\n\t\t\t\t\tDatasets\n\t\t\t\t\tSpaces\n\t\t\t\t\tPosts\n\t\t\t\t\tDocs\n\t\t\tSolutions\nPricing\nLog In\nSign Up\nmicrosoft\n/\nPhi-3-vision-128k-instruct\nlike\n813\nText Generation\nTransformers\nSafetensors\nmultilingual\nphi3_v\nnlp\ncode\nvision\nconversational\ncustom_code\nLicense:\nmit\n\t\t\tModel card\nFiles\nFiles and versions\n\t\t\tCommunity\n\t\t\t54\n\t\t\tTrain\n\t\t\tUse this model\n\t\t\t\t\t\tEdit model card\nModel Summary\nIntended Uses\nHow to Use\nChat Format\nSample inference code\nHow to finetune?\nResponsible AI Considerations\nTraining\nModel\nDatasets\nBenchmarks\nSoftware\nHardware\nLicense\nTrademarks\n\t\tModel Summary\nThe Phi-3-Vision-128K-Instruct is a lightweight, state-of-the-art open multimodal model built upon datasets which include - synthetic data and filtered publicly available websites - with a focus on very high-quality, reasoning dense data both on text and vision. The model belongs to the Phi-3 model family, and the multimodal version comes with 128K context length (in tokens) it can support. The model underwent a rigorous enhancement process, incorporating both supervised fine-tuning and direct preference optimization to ensure precise instruction adherence and robust safety measures.\nResources and Technical Documentation:\nPhi-3 Microsoft Blog\nPhi-3 Technical Report\nPhi-3 on Azure AI Studio\nPhi-3 Cookbook\nShort Context\nLong Context\nMini\n4K [HF] ; [ONNX] ; [GGUF]\n128K [HF] ; [ONNX]\nSmall\n8K [HF] ; [ONNX]\n128K [HF] ; [ONNX]\nMedium\n4K [HF] ; [ONNX]\n128K [HF] ; [ONNX]\nVision\n128K [HF] ; [ONNX]\n\t\tIntended Uses\nPrimary use cases\nThe model is intended for broad commercial and research use in English. The model provides uses for general purpose AI systems and applications with visual and text input capabilities which require \nmemory/compute constrained environments;\nlatency bound scenarios;\ngeneral image understanding;\nOCR;\nchart and table understanding.\nOur model is designed to accelerate research on efficient language and multimodal models, for use as a building block for generative AI powered features.\nUse case considerations\nOur models are not specifically designed or evaluated for all downstream purposes. Developers should consider common limitations of language models as they select use cases, and evaluate and mitigate for accuracy, safety, and fairness before using within a specific downstream use case, particularly for high-risk scenarios. \nDevelopers should be aware of and adhere to applicable laws or regulations (including privacy, trade compliance laws, etc.) that are relevant to their use case. \nNothing contained in this Model Card should be interpreted as or deemed a restriction or modification to the license the model is released under.\n\t\tHow to Use\nPhi-3-Vision-128K-Instruct has been integrated in the development version (4.40.2) of transformers. Until the official version is released through pip, ensure that you are doing one of the following:\nWhen loading the model, ensure that trust_remote_code=True is passed as an argument of the from_pretrained() function.\nUpdate your local transformers to the development version: pip uninstall -y transformers && pip install git+https://github.com/huggingface/transformers. The previous command is an alternative to cloning and installing from the source.\nThe current transformers version can be verified with: pip list | grep transformers.\nExamples of required packages:\nflash_attn==2.5.8\nnumpy==1.24.4\nPillow==10.3.0\nRequests==2.31.0\ntorch==2.3.0\ntorchvision==0.18.0\ntransformers==4.40.2\nPhi-3-Vision-128K-Instruct is also available in Azure AI Studio.\n\t\tChat Format\nGiven the nature of the training data, the Phi-3-Vision-128K-Instruct model is best suited for a single image input wih prompts using the chat format as follows. \nYou can provide the prompt as a single image with a generic template as follow:\n<|user|>\\n<|image_1|>\\n{prompt}<|end|>\\n<|assistant|>\\n \nwhere the model generates the text after <|assistant|> . In case of multi-turn conversation, the prompt can be formatted as follows:\n<|user|>\\n<|image_1|>\\n{prompt_1}<|end|>\\n<|assistant|>\\n{response_1}<|end|>\\n<|user|>\\n{prompt_2}<|end|>\\n<|assistant|>\\n \n\t\tSample inference code\nThis code snippets show how to get quickly started with running the model on a GPU:\nfrom PIL import Image \nimport requests \nfrom transformers import AutoModelForCausalLM \nfrom transformers import AutoProcessor \nmodel_id = \"microsoft/Phi-3-vision-128k-instruct\" \nmodel = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"cuda\", trust_remote_code=True, torch_dtype=\"auto\", _attn_implementation='flash_attention_2') # use _attn_implementation='eager' to disable flash attention\nprocessor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True) \nmessages = [ {\"role\": \"user\", \"content\": \"<|image_1|>\\nWhat is shown in this image?\"}, {\"role\": \"assistant\", \"content\": \"The chart displays the percentage of respondents who agree with various statements about their preparedness for meetings. It shows five categories: 'Having clear and pre-defined goals for meetings', 'Knowing where to find the information I need for a meeting', 'Understanding my exact role and responsibilities when I'm invited', 'Having tools to manage admin tasks like note-taking or summarization', and 'Having more focus time to sufficiently prepare for meetings'. Each category has an associated bar indicating the level of agreement, measured on a scale from 0% to 100%.\"}, {\"role\": \"user\", \"content\": \"Provide insightful questions to spark discussion.\"} \n] \nurl = \"https://assets-c4akfrf5b4d3f4b7.z01.azurefd.net/assets/2024/04/BMDataViz_661fb89f3845e.png\" \nimage = Image.open(requests.get(url, stream=True).raw) \nprompt = processor.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\ninputs = processor(prompt, [image], return_tensors=\"pt\").to(\"cuda:0\") \ngeneration_args = { \"max_new_tokens\": 500, \"temperature\": 0.0, \"do_sample\": False, \n} \ngenerate_ids = model.generate(**inputs, eos_token_id=processor.tokenizer.eos_token_id, **generation_args) \n# remove input tokens \ngenerate_ids = generate_ids[:, inputs['input_ids'].shape[1]:]\nresponse = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0] \nprint(response) \nAdditional basic examples are provided here.\n\t\tHow to finetune?\nWe recommend user to take a look at the Phi-3 CookBook finetuning recipe for Vision\n\t\tResponsible AI Considerations\nLike other models, the Phi family of models can potentially behave in ways that are unfair, unreliable, or offensive. Some of the limiting behaviors to be aware of include: \nQuality of Service: The Phi models are trained primarily on English text. Languages other than English will experience worse performance. English language varieties with less representation in the training data might experience worse performance than standard American English. \nRepresentation of Harms & Perpetuation of Stereotypes: These models can over- or under-represent groups of people, erase representation of some groups, or reinforce demeaning or negative stereotypes. Despite safety post-training, these limitations may still be present due to differing levels of representation of different groups or prevalence of examples of negative stereotypes in training data that reflect real-world patterns and societal biases. \nInappropriate or Offensive Content: These models may produce other types of inappropriate or offensive content, which may make it inappropriate to deploy for sensitive contexts without additional mitigations that are specific to the use case. \nInformation Reliability: Language models can generate nonsensical content or fabricate content that might sound reasonable but is inaccurate or outdated. \nLimited Scope for Code: Majority of Phi-3 training data is based in Python and use common packages such as \"typing, math, random, collections, datetime, itertools\". If the model generates Python scripts that utilize other packages or scripts in other languages, we strongly recommend users manually verify all API uses.\nDevelopers should apply responsible AI best practices and are responsible for ensuring that a specific use case complies with relevant laws and regulations (e.g. privacy, trade, etc.). Important areas for consideration include: \nAllocation: Models may not be suitable for scenarios that could have consequential impact on legal status or the allocation of resources or life opportunities (ex: housing, employment, credit, etc.) without further assessments and additional debiasing techniques.\nHigh-Risk Scenarios: Developers should assess suitability of using models in high-risk scenarios where unfair, unreliable or offensive outputs might be extremely costly or lead to harm. This includes providing advice in sensitive or expert domains where accuracy and reliability are critical (ex: legal or health advice). Additional safeguards should be implemented at the application level according to the deployment context.\nMisinformation: Models may produce inaccurate information. Developers should follow transparency best practices and inform end-users they are interacting with an AI system. At the application level, developers can build feedback mechanisms and pipelines to ground responses in use-case specific, contextual information, a technique known as Retrieval Augmented Generation (RAG).\nGeneration of Harmful Content: Developers should assess outputs for their context and use available safety classifiers or custom solutions appropriate for their use case.\nMisuse: Other forms of misuse such as fraud, spam, or malware production may be possible, and developers should ensure that their applications do not violate applicable laws and regulations.\nIdentification of individuals: models with vision capabilities may have the potential to uniquely identify individuals in images. Safety post-training steers the model to refuse such requests, but developers should consider and implement, as appropriate, additional mitigations or user consent flows as required in their respective jurisdiction, (e.g., building measures to blur faces in image inputs before processing.\n\t\tTraining\n\t\tModel\nArchitecture: Phi-3-Vision-128K-Instruct has 4.2B parameters and contains image encoder, connector, projector, and Phi-3 Mini language model.\nInputs: Text and Image. It\u2019s best suited for prompts using the chat format. \nContext length: 128K tokens\nGPUs: 512 H100-80G\nTraining time: 1.5 days\nTraining data: 500B vision and text tokens\nOutputs: Generated text in response to the input\nDates: Our models were trained between February and April 2024\nStatus: This is a static model trained on an offline text dataset with cutoff date Mar 15, 2024. Future versions of the tuned models may be released as we improve models.\nRelease Type: Open weight release\nRelease dates: The model weight is released on May 21, 2024.\n\t\tDatasets\nOur training data includes a wide variety of sources, and is a combination of \npublicly available documents filtered rigorously for quality, selected high-quality educational data and code;\nselected high-quality image-text interleave;\nnewly created synthetic, \u201ctextbook-like\u201d data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (science, daily activities, theory of mind, etc.), newly created image data, e.g., chart/table/diagram/slides;\nhigh quality chat format supervised data covering various topics to reflect human preferences on different aspects such as instruct-following, truthfulness, honesty and helpfulness.\nThe data collection process involved sourcing information from publicly available documents, with a meticulous approach to filtering out undesirable documents and images. To safeguard privacy, we carefully filtered various image and text data sources to remove or scrub any potentially personal data from the training data.\nMore details can be found in the Phi-3 Technical Report.\n\t\tBenchmarks\nTo understand the capabilities, we compare Phi-3-Vision-128K-Instruct with a set of models over a variety of zero-shot benchmarks using our internal benchmark platform.\nBenchmark\nPhi-3 Vision-128K-In\nLlaVA-1.6 Vicuna-7B\nQWEN-VL Chat\nLlama3-Llava-Next-8B\nClaude-3 Haiku\nGemini 1.0 Pro V\nGPT-4V-Turbo\nMMMU\n40.4\n34.2\n39.0\n36.4\n40.7\n42.0\n55.5\nMMBench\n80.5\n76.3\n75.8\n79.4\n62.4\n80.0\n86.1\nScienceQA\n90.8\n70.6\n67.2\n73.7\n72.0\n79.7\n75.7\nMathVista\n44.5\n31.5\n29.4\n34.8\n33.2\n35.0\n47.5\nInterGPS\n38.1\n20.5\n22.3\n24.6\n32.1\n28.6\n41.0\nAI2D\n76.7\n63.1\n59.8\n66.9\n60.3\n62.8\n74.7\nChartQA\n81.4\n55.0\n50.9\n65.8\n59.3\n58.0\n62.3\nTextVQA\n70.9\n64.6\n59.4\n55.7\n62.7\n64.7\n68.1\nPOPE\n85.8\n87.2\n82.6\n87.0\n74.4\n84.2\n83.7\n\t\tSoftware\nPyTorch\nTransformers\nFlash-Attention\n\t\tHardware\nNote that by default, the Phi-3-Vision-128K model uses flash attention, which requires certain types of GPU hardware to run. We have tested on the following GPU types:\nNVIDIA A100\nNVIDIA A6000\nNVIDIA H100\n\t\tLicense\nThe model is licensed under the MIT license.\n\t\tTrademarks\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow\u202fMicrosoft\u2019s Trademark & Brand Guidelines. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party\u2019s policies.\nDownloads last month196,820\nSafetensors\nModel size\n4.15B params\nTensor type\nBF16\n\t\t\t\t\t\t\u00b7\nText Generation\nInference API (serverless) does not yet support model repos that contain custom code.\n\t\t\t\t\t\tSpaces using\n\t\t\t\t\t\tmicrosoft/Phi-3-vision-128k-instruct\n26\n\ud83d\ude3b\nysharma/Microsoft_Phi-3-Vision-128k\ud83d\ude3b\nprithivMLmods/PHI3-VISION\ud83d\udcda\naifeifei798/nemotron-4-340b-instruct\ud83c\udfc3\nsimonraj/PersonalTrainer-Arnold\ud83d\udcda\naifeifei798/Mistral-NeMo-12B-Instruct\ud83c\udf10\ndwb2023/hf_extractor\ud83d\ude3b\nTypeGPT/Microsoft_Phi-3-Vision-128k\ud83d\udcc8\nbaconnier/BANKER_Vision\ud83d\ude80\ndwb2023/model_explorer4\ud83d\udd2c\ud83e\uddeb\ndwb2023/omniscience\ud83d\udcda\naifeifei798/Llama-3-70B-Instruct\ud83d\udd25\nMaziyarPanahi/Phi-3-vision-128k\ud83d\ude80\ndwb2023/model_explorer2\ud83d\ude3b\nawacke1/ImageChatbot-Phi-3-Vision-128k\ud83c\udf16\nSharryOG/AI\ud83d\udd25\nhf1024/Phi-3-vision-128k\ud83d\ude3b\nnikhilalgorithma/vision1\ud83d\ude3b\naggaranu/Microsoft_Phi-3-Vision-128k\ud83d\ude3b\nzohadev/Microsoft-Phi-3-Vision-128k\ud83d\udd25\nShankarm08/phi3vision\n+ 21 Spaces\n+ 6 Spaces\n\t\t\t\t\t\tCollection including\n\t\t\t\t\t\tmicrosoft/Phi-3-vision-128k-instruct\nPhi-3\n\t\t\t\t\tCollection\nPhi-3 family of small language and multi-modal models. Language models are available in short- and long-context lengths.\n\u2022 \n23 items\n\u2022 \nUpdated\n\t\t\t\t\t10 days ago\n\u2022\n\t\t\t\t\t372\nCompany\n\u00a9 Hugging Face\nTOS\nPrivacy\nAbout\nJobs\nWebsite\nModels\nDatasets\nSpaces\nPricing\nDocs\"\n\nThis is the scraped text from the provided URL."}
{"agent": "scrape_agent", "event": "agent_step", "time": "2024-07-21 17:55:39.043877+05:30", "tool": "Read website content", "tool_input": "{\"website_url\": \"https://huggingface.co/microsoft/Phi-3-vision-128k-instruct\"}", "log": "Thought: I need to scrape the content from the provided URLs.\n\nAction: Read website content\nAction Input: {\"website_url\": \"https://huggingface.co/microsoft/Phi-3-vision-128k-instruct\"}"}
{"agent": "scrape_agent", "event": "agent_step", "time": "2024-07-21 17:55:40.382956+05:30", "tool": "Read website content", "tool_input": "{\"website_url\": \"https://huggingface.co/microsoft/Phi-3-vision-128k-instruct\"}", "log": "Thought: I need to scrape the content from the provided URL.\n\nAction: Read website content\nAction Input: {\"website_url\": \"https://huggingface.co/microsoft/Phi-3-vision-128k-instruct\"}"}
{"agent": "scrape_agent", "event": "agent_step", "time": "2024-07-21 17:56:20.567169+05:30", "tool": "None (no action is needed, as the text is already extracted)", "tool_input": "None", "log": "Thought: I need to extract the text from the observed content.\n\nAction: None (no action is needed, as the text is already extracted)\n\nAction Input: None"}
{"agent": "scrape_agent", "event": "agent_step", "time": "2024-07-21 17:56:34.160740+05:30", "tool": "Read website content", "tool_input": "{\"website_url\": \"https://huggingface.co/microsoft/Phi-3-vision-128k-instruct\"}", "log": "Thought: I need to scrape the content from the provided URL.\n\nAction: Read website content\nAction Input: {\"website_url\": \"https://huggingface.co/microsoft/Phi-3-vision-128k-instruct\"}"}
{"agent": "scrape_agent", "event": "agent_step", "time": "2024-07-21 17:56:48.050755+05:30", "tool": "Read website content", "tool_input": "{\"website_url\": \"https://huggingface.co/microsoft/Phi-3-vision-128k-instruct\"}", "log": "Thought: I need to scrape the content from the provided URL.\n\nAction: Read website content\nAction Input: {\"website_url\": \"https://huggingface.co/microsoft/Phi-3-vision-128k-instruct\"}"}
{"agent": "scrape_agent", "event": "agent_step", "time": "2024-07-21 17:57:01.373369+05:30", "tool": "Read website content", "tool_input": "{\"website_url\": \"https://huggingface.co/microsoft/Phi-3-vision-128k-instruct\"}", "log": "Thought: I need to scrape the content from the provided URL.\n\nAction: Read website content\nAction Input: {\"website_url\": \"https://huggingface.co/microsoft/Phi-3-vision-128k-instruct\"}"}
{"agent": "scrape_agent", "event": "agent_step", "time": "2024-07-21 17:57:14.123941+05:30", "tool": "Read website content", "tool_input": "{\"website_url\": \"https://huggingface.co/microsoft/Phi-3-vision-128k-instruct\"}", "log": "Thought: I need to scrape the content from the provided URL.\n\nAction: Read website content\nAction Input: {\"website_url\": \"https://huggingface.co/microsoft/Phi-3-vision-128k-instruct\"}"}
{"agent": "scrape_agent", "event": "agent_step", "time": "2024-07-21 17:57:15.577030+05:30", "tool": "Read website content", "tool_input": "{\"website_url\": \"https://huggingface.co/microsoft/Phi-3-vision-128k-instruct\"}", "log": "Thought: I need to scrape the content from the provided URL.\n\nAction: Read website content\nAction Input: {\"website_url\": \"https://huggingface.co/microsoft/Phi-3-vision-128k-instruct\"}"}
{"agent": "scrape_agent", "event": "agent_finish", "time": "2024-07-21 17:58:13.530069+05:30", "tool": "Unknown", "tool_input": "Unknown", "log": "Thought: I now know the final answer\nFinal Answer: The scraped text from the provided URL is:\n\nmicrosoft/Phi-3-vision-128k-instruct \u00b7 Hugging Face\nHugging Face\n\t\t\t\t\tModels\n\t\t\t\t\tDatasets\n\t\t\t\t\tSpaces\n\t\t\t\t\tPosts\n\t\t\t\t\tDocs\n\t\t\tSolutions\nPricing\nLog In\nSign Up\nmicrosoft\n/\nPhi-3-vision-128k-instruct\nlike\n813\nText Generation\nTransformers\nSafetensors\nmultilingual\nphi3_v\nnlp\ncode\nvision\nconversational\ncustom_code\nLicense:\nmit\n\t\t\tModel card\nFiles\nFiles and versions\n\t\t\tCommunity\n\t\t\t54\n\t\t\tTrain\n\t\t\tUse this model\n\t\t\t\t\t\tEdit model card\nModel Summary\nIntended Uses\nHow to Use\nChat Format\nSample inference code\nHow to finetune?\nResponsible AI Considerations\nTraining\nModel\nDatasets\nBenchmarks\nSoftware\nHardware\nLicense\nTrademarks\n\t\tModel Summary\nThe Phi-3-Vision-128K-Instruct is a lightweight, state-of-the-art open multimodal model built upon datasets which include - synthetic data and filtered publicly available websites - with a focus on very high-quality, reasoning dense data both on text and vision. The model belongs to the Phi-3 model family, and the multimodal version comes with 128K context length (in tokens) it can support. The model underwent a rigorous enhancement process, incorporating both supervised fine-tuning and direct preference optimization to ensure precise instruction adherence and robust safety measures.\nResources and Technical Documentation:\nPhi-3 Microsoft Blog\nPhi-3 Technical Report\nPhi-3 on Azure AI Studio\nPhi-3 Cookbook\nShort Context\nLong Context\nMini\n4K [HF] ; [ONNX] ; [GGUF]\n128K [HF] ; [ONNX]\nSmall\n8K [HF] ; [ONNX]\n128K [HF] ; [ONNX]\nMedium\n4K [HF] ; [ONNX]\n128K [HF] ; [ONNX]\nVision\n128K [HF] ; [ONNX]\n\t\tIntended Uses\nPrimary use cases\nThe model is intended for broad commercial and research use in English. The model provides uses for general purpose AI systems and applications with visual and text input capabilities which require \nmemory/compute constrained environments;\nlatency bound scenarios;\ngeneral image understanding;\nOCR;\nchart and table understanding.\nOur model is designed to accelerate research on efficient language and multimodal models, for use as a building block for generative AI powered features.\nUse case considerations\nOur models are not specifically designed or evaluated for all downstream purposes. Developers should consider common limitations of language models as they select use cases, and evaluate and mitigate for accuracy, safety, and fairness before using within a specific downstream use case, particularly for high-risk scenarios. \nDevelopers should be aware of and adhere to applicable laws or regulations (including privacy, trade compliance laws, etc.) that are relevant to their use case. \nNothing contained in this Model Card should be interpreted as or deemed a restriction or modification to the license the model is released under.\n\t\tHow to Use\nPhi-3-Vision-128K-Instruct has been integrated in the development version (4.40.2) of transformers. Until the official version is released through pip, ensure that you are doing one of the following:\nWhen loading the model, ensure that trust_remote_code=True is passed as an argument of the from_pretrained() function.\nUpdate your local transformers to the development version: pip uninstall -y transformers && pip install git+https://github.com/huggingface/transformers. The previous command is an alternative to cloning and installing from the source.\nThe current transformers version can be verified with: pip list | grep transformers.\nExamples of required packages:\nflash_attn==2.5.8\nnumpy==1.24.4\nPillow==10.3.0\nRequests==2.31.0\ntorch==2.3.0\ntorchvision==0.18.0\ntransformers==4.40.2\nPhi-3-Vision-128K-Instruct is also available in Azure AI Studio.\n\t\tChat Format\nGiven the nature of the training data, the Phi-3-Vision-128K-Instruct model is best suited for a single image input wih prompts using the chat format as follows. \nYou can provide the prompt as a single image with a generic template as follow:\n<|user|>\\n<|image_1|>\\n{prompt}<|end|>\\n<|assistant|>\\n \nwhere the model generates the text after <|assistant|> . In case of multi-turn conversation, the prompt can be formatted as follows:\n<|user|>\\n<|image_1|>\\n{prompt_1}<|end|>\\n<|assistant|>\\n{response_1}<|end|>\\n<|user|>\\n{prompt_2}<|end|>\\n<|assistant|>\\n \n\t\tSample inference code\nThis code snippets show how to get quickly started with running the model on a GPU:\nfrom PIL import Image \nimport requests \nfrom transformers import AutoModelForCausalLM \nfrom transformers import AutoProcessor \nmodel_id = \"microsoft/Phi-3-vision-128k-instruct\" \nmodel = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"cuda\", trust_remote_code=True, torch_dtype=\"auto\", _attn_implementation='flash_attention_2') # use _attn_implementation='eager' to disable flash attention\nprocessor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True) \nmessages = [ {\"role\": \"user\", \"content\": \"<|image_1|>\\nWhat is shown in this image?\"}, {\"role\": \"assistant\", \"content\": \"The chart displays the percentage of respondents who agree with various statements about their preparedness for meetings. It shows five categories: 'Having clear and pre-defined goals for meetings', 'Knowing where to find the information I need for a meeting', 'Understanding my exact role and responsibilities when I'm invited', 'Having tools to manage admin tasks like note-taking or summarization', and 'Having more focus time to sufficiently prepare for meetings'. Each category has an associated bar indicating the level of agreement, measured on a scale from 0% to 100%.\"}, {\"role\": \"user\", \"content\": \"Provide insightful questions to spark discussion.\"} \n] \nurl = \"https://assets-c4akfrf5b4d3f4b7.z01.azurefd.net/assets/2024/04/BMDataViz_661fb89f3845e.png\" \nimage = Image.open(requests.get(url, stream=True).raw) \nprompt = processor.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\ninputs = processor(prompt, [image], return_tensors=\"pt\").to(\"cuda:0\") \ngeneration_args = { \"max_new_tokens\": 500, \"temperature\": 0.0, \"do_sample\": False, \n} \ngenerate_ids = model.generate(**inputs, eos_token_id=processor.tokenizer.eos_token_id, **generation_args) \n# remove input tokens \ngenerate_ids = generate_ids[:, inputs['input_ids'].shape[1]:]\nresponse = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0] \nprint(response) \nAdditional basic examples are provided here.\n\t\tHow to finetune?\nWe recommend user to take a look at the Phi-3 CookBook finetuning recipe for Vision\n\t\tResponsible AI Considerations\nLike other models, the Phi family of models can potentially behave in ways that are unfair, unreliable, or offensive. Some of the limiting behaviors to be aware of include: \nQuality of Service: The Phi models are trained primarily on English text. Languages other than English will experience worse performance. English language varieties with less representation in the training data might experience worse performance than standard American English. \nRepresentation of Harms & Perpetuation of Stereotypes: These models can over- or under-represent groups of people, erase representation of some groups, or reinforce demeaning or negative stereotypes. Despite safety post-training, these limitations may still be present due to differing levels of representation of different groups or prevalence of examples of negative stereotypes in training data that reflect real-world patterns and societal biases. \nInappropriate or Offensive Content: These models may produce other types of inappropriate or offensive content, which may make it inappropriate to deploy for sensitive contexts without additional mitigations that are specific to the use case. \nInformation Reliability: Language models can generate nonsensical content or fabricate content that might sound reasonable but is inaccurate or outdated. \nLimited Scope for Code: Majority of Phi-3 training data is based in Python and use common packages such as \"typing, math, random, collections, datetime, itertools\". If the model generates Python scripts that utilize other packages or scripts in other languages, we strongly recommend users manually verify all API uses.\nDevelopers should apply responsible AI best practices and are responsible for ensuring that a specific use case complies with relevant laws and regulations (e.g. privacy, trade, etc.). Important areas for consideration include: \nAllocation: Models may not be suitable for scenarios that could have consequential impact on legal status or the allocation of resources or life opportunities (ex: housing, employment, credit, etc.) without further assessments and additional debiasing techniques.\nHigh-Risk Scenarios: Developers should assess suitability of using models in high-risk scenarios where unfair, unreliable or offensive outputs might be extremely costly or lead to harm. This includes providing advice in sensitive or expert domains where accuracy and reliability are critical (ex: legal or health advice). Additional safeguards should be implemented at the application level according to the deployment context.\nMisinformation: Models may produce inaccurate information. Developers should follow transparency best practices and inform end-users they are interacting with an AI system. At the application level, developers can build feedback mechanisms and pipelines to ground responses in use-case specific, contextual information, a technique known as Retrieval Augmented Generation (RAG).\nGeneration of Harmful Content: Developers should assess outputs for their context and use available safety classifiers or custom solutions appropriate for their use case.\nMisuse: Other forms of misuse such as fraud, spam, or malware production may be possible, and developers should ensure that their applications do not violate applicable laws and regulations.\nIdentification of individuals: models with vision capabilities may have the potential to uniquely identify individuals in images. Safety post-training steers the model to refuse such requests, but developers should consider and implement, as appropriate, additional mitigations or user consent flows as required in their respective jurisdiction, (e.g., building measures to blur faces in image inputs before processing.\n\t\tTraining\n\t\tModel\nArchitecture: Phi-3-Vision-128K-Instruct has 4.2B parameters and contains image encoder, connector, projector, and Phi-3 Mini language model.\nInputs: Text and Image. It\u2019s best suited for prompts using the chat format. \nContext length: 128K tokens\nGPUs: 512 H100-80G\nTraining time: 1.5 days\nTraining data: 500B vision and text tokens\nOutputs: Generated text in response to the input\nDates: Our models were trained between February and April 2024\nStatus: This is a static model trained on an offline text dataset with cutoff date Mar 15, 2024. Future versions of the tuned models may be released as we improve models.\nRelease Type: Open weight release\nRelease dates: The model weight is released on May 21, 2024.\n\t\tDatasets\nOur training data includes a wide variety of sources, and is a combination of \npublicly available documents filtered rigorously for quality, selected high-quality educational data and code;\nselected high-quality image-text interleave;\nnewly created synthetic, \u201ctextbook-like\u201d data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (science, daily activities, theory of mind, etc.), newly created image data, e.g., chart/table/diagram/slides;\nhigh quality chat format supervised data covering various topics to reflect human preferences on different aspects such as instruct-following, truthfulness, honesty and helpfulness.\nThe data collection process involved sourcing information from publicly available documents, with a meticulous approach to filtering out undesirable documents and images. To safeguard privacy, we carefully filtered various image and text data sources to remove or scrub any potentially personal data from the training data.\nMore details can be found in the Phi-3 Technical Report.\n\t\tBenchmarks\nTo understand the capabilities, we compare Phi-3-Vision-128K-Instruct with a set of models over a variety of zero-shot benchmarks using our internal benchmark platform.\nBenchmark\nPhi-3 Vision-128K-In\nLlaVA-1.6 Vicuna-7B\nQWEN-VL Chat\nLlama3-Llava-Next-8B\nClaude-3 Haiku\nGemini 1.0 Pro V\nGPT-4V-Turbo\nMMMU\n40.4\n34.2\n39.0\n36.4\n40.7\n42.0\n55.5\nMMBench\n80.5\n76.3\n75.8\n79.4\n62.4\n80.0\n86.1\nScienceQA\n90.8\n70.6\n67.2\n73.7\n72.0\n79.7\n75.7\nMathVista\n44.5\n31.5\n29.4\n34.8\n33.2\n35.0\n47.5\nInterGPS\n38.1\n20.5\n22.3\n24.6\n32.1\n28.6\n41.0\nAI2D\n76.7\n63.1\n59.8\n66.9\n60.3\n62.8\n74.7\nChartQA\n81.4\n55.0\n50.9\n65.8\n59.3\n58.0\n62.3\nTextVQA\n70.9\n64.6\n59.4\n55.7\n62.7\n64.7\n68.1\nPOPE\n85.8\n87.2\n82.6\n87.0\n74.4\n84.2\n83.7\n\t\tSoftware\nPyTorch\nTransformers\nFlash-Attention\n\t\tHardware\nNote that by default, the Phi-3-Vision-128K model uses flash attention, which requires certain types of GPU hardware to run. We have tested on the following GPU types:\nNVIDIA A100\nNVIDIA A6000\nNVIDIA H100\n\t\tLicense\nThe model is licensed under the MIT license.\n\t\tTrademarks\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow\u202fMicrosoft"}
{"agent": "scrape_agent", "event": "agent_step", "time": "2024-07-22 20:35:45.716512+05:30", "tool": "Read website content", "tool_input": "{\"website_url\": \"https://huggingface.co/microsoft/Phi-3-vision-128k-instruct\"}", "log": "Thought: I need to scrape the content from the provided URL using the `scrape_tool` and extract the text.\n\nAction: Read website content\nAction Input: {\"website_url\": \"https://huggingface.co/microsoft/Phi-3-vision-128k-instruct\"}"}
{"agent": "scrape_agent", "event": "agent_step", "time": "2024-07-22 20:35:53.530304+05:30", "tool": "Read website content", "tool_input": "{\"website_url\": \"https://huggingface.co/microsoft/Phi-3-vision-128k-instruct\"}", "log": "Thought: I have successfully scraped the content from the provided URL.\n\nAction: Read website content\nAction Input: {\"website_url\": \"https://huggingface.co/microsoft/Phi-3-vision-128k-instruct\"}"}
{"agent": "scrape_agent", "event": "agent_finish", "time": "2024-07-22 20:36:36.761287+05:30", "tool": "Unknown", "tool_input": "Unknown", "log": "Thought: I now know the final answer\n\nFinal Answer: \nThe scraped text from the provided URL is:\n\n\"microsoft/Phi-3-vision-128k-instruct \u00b7 Hugging Face\nHugging Face\n\t\t\t\t\tModels\n\t\t\t\t\tDatasets\n\t\t\t\t\tSpaces\n\t\t\t\t\tPosts\n\t\t\t\t\tDocs\n\t\t\tSolutions\nPricing\nLog In\nSign Up\nmicrosoft\n/\nPhi-3-vision-128k-instruct\nlike\n816\nText Generation\nTransformers\nSafetensors\nmultilingual\nphi3_v\nnlp\ncode\nvision\nconversational\ncustom_code\nLicense:\nmit\n\t\t\tModel card\nFiles\nFiles and versions\n\t\t\tCommunity\n\t\t\t54\n\t\t\tTrain\n\t\t\tUse this model\n\t\t\t\t\t\tEdit model card\nModel Summary\nIntended Uses\nHow to Use\nChat Format\nSample inference code\nHow to finetune?\nResponsible AI Considerations\nTraining\nModel\nDatasets\nBenchmarks\nSoftware\nHardware\nLicense\nTrademarks\n\t\tModel Summary\nThe Phi-3-Vision-128K-Instruct is a lightweight, state-of-the-art open multimodal model built upon datasets which include - synthetic data and filtered publicly available websites - with a focus on very high-quality, reasoning dense data both on text and vision. The model belongs to the Phi-3 model family, and the multimodal version comes with 128K context length (in tokens) it can support. The model underwent a rigorous enhancement process, incorporating both supervised fine-tuning and direct preference optimization to ensure precise instruction adherence and robust safety measures.\nResources and Technical Documentation:\nPhi-3 Microsoft Blog\nPhi-3 Technical Report\nPhi-3 on Azure AI Studio\nPhi-3 Cookbook\nShort Context\nLong Context\nMini\n4K [HF] ; [ONNX] ; [GGUF]\n128K [HF] ; [ONNX]\nSmall\n8K [HF] ; [ONNX]\n128K [HF] ; [ONNX]\nMedium\n4K [HF] ; [ONNX]\n128K [HF] ; [ONNX]\nVision\n128K [HF] ; [ONNX]\n\t\tIntended Uses\nPrimary use cases\nThe model is intended for broad commercial and research use in English. The model provides uses for general purpose AI systems and applications with visual and text input capabilities which require \nmemory/compute constrained environments;\nlatency bound scenarios;\ngeneral image understanding;\nOCR;\nchart and table understanding.\nOur model is designed to accelerate research on efficient language and multimodal models, for use as a building block for generative AI powered features.\nUse case considerations\nOur models are not specifically designed or evaluated for all downstream purposes. Developers should consider common limitations of language models as they select use cases, and evaluate and mitigate for accuracy, safety, and fairness before using within a specific downstream use case, particularly for high-risk scenarios. \nDevelopers should be aware of and adhere to applicable laws or regulations (including privacy, trade compliance laws, etc.) that are relevant to their use case. \nNothing contained in this Model Card should be interpreted as or deemed a restriction or modification to the license the model is released under.\n\t\tHow to Use\nPhi-3-Vision-128K-Instruct has been integrated in the development version (4.40.2) of transformers. Until the official version is released through pip, ensure that you are doing one of the following:\nWhen loading the model, ensure that trust_remote_code=True is passed as an argument of the from_pretrained() function.\nUpdate your local transformers to the development version: pip uninstall -y transformers && pip install git+https://github.com/huggingface/transformers. The previous command is an alternative to cloning and installing from the source.\nThe current transformers version can be verified with: pip list | grep transformers.\nExamples of required packages:\nflash_attn==2.5.8\nnumpy==1.24.4\nPillow==10.3.0\nRequests==2.31.0\ntorch==2.3.0\ntorchvision==0.18.0\ntransformers==4.40.2\nPhi-3-Vision-128K-Instruct is also available in Azure AI Studio.\n\t\tChat Format\nGiven the nature of the training data, the Phi-3-Vision-128K-Instruct model is best suited for a single image input wih prompts using the chat format as follows. \nYou can provide the prompt as a single image with a generic template as follow:\n<|user|>\\n<|image_1|>\\n{prompt}<|end|>\\n<|assistant|>\\n \nwhere the model generates the text after <|assistant|> . In case of multi-turn conversation, the prompt can be formatted as follows:\n<|user|>\\n<|image_1|>\\n{prompt_1}<|end|>\\n<|assistant|>\\n{response_1}<|end|>\\n<|user|>\\n{prompt_2}<|end|>\\n<|assistant|>\\n \n\t\tSample inference code\nThis code snippets show how to get quickly started with running the model on a GPU:\nfrom PIL import Image \nimport requests \nfrom transformers import AutoModelForCausalLM \nfrom transformers import AutoProcessor \nmodel_id = \"microsoft/Phi-3-vision-128k-instruct\" \nmodel = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"cuda\", trust_remote_code=True, torch_dtype=\"auto\", _attn_implementation='flash_attention_2') # use _attn_implementation='eager' to disable flash attention\nprocessor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True) \nmessages = [ {\"role\": \"user\", \"content\": \"<|image_1|>\\nWhat is shown in this image?\"}, {\"role\": \"assistant\", \"content\": \"The chart displays the percentage of respondents who agree with various statements about their preparedness for meetings. It shows five categories: 'Having clear and pre-defined goals for meetings', 'Knowing where to find the information I need for a meeting', 'Understanding my exact role and responsibilities when I'm invited', 'Having tools to manage admin tasks like note-taking or summarization', and 'Having more focus time to sufficiently prepare for meetings'. Each category has an associated bar indicating the level of agreement, measured on a scale from 0% to 100%.\"}, {\"role\": \"user\", \"content\": \"Provide insightful questions to spark discussion.\"} \n] \nurl = \"https://assets-c4akfrf5b4d3f4b7.z01.azurefd.net/assets/2024/04/BMDataViz_661fb89f3845e.png\" \nimage = Image.open(requests.get(url, stream=True).raw) \nprompt = processor.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\ninputs = processor(prompt, [image], return_tensors=\"pt\").to(\"cuda:0\") \ngeneration_args = { \"max_new_tokens\": 500, \"temperature\": 0.0, \"do_sample\": False, \n} \ngenerate_ids = model.generate(**inputs, eos_token_id=processor.tokenizer.eos_token_id, **generation_args) \n# remove input tokens \ngenerate_ids = generate_ids[:, inputs['input_ids'].shape[1]:]\nresponse = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0] \nprint(response) \nAdditional basic examples are provided here.\n\t\tHow to finetune?\nWe recommend user to take a look at the Phi-3 CookBook finetuning recipe for Vision\n\t\tResponsible AI Considerations\nLike other models, the Phi family of models can potentially behave in ways that are unfair, unreliable, or offensive. Some of the limiting behaviors to be aware of include: \nQuality of Service: The Phi models are trained primarily on English text. Languages other than English will experience worse performance. English language varieties with less representation in the training data might experience worse performance than standard American English. \nRepresentation of Harms & Perpetuation of Stereotypes: These models can over- or under-represent groups of people, erase representation of some groups, or reinforce demeaning or negative stereotypes. Despite safety post-training, these limitations may still be present due to differing levels of representation of different groups or prevalence of examples of negative stereotypes in training data that reflect real-world patterns and societal biases. \nInappropriate or Offensive Content: These models may produce other types of inappropriate or offensive content, which may make it inappropriate to deploy for sensitive contexts without additional mitigations that are specific to the use case. \nInformation Reliability: Language models can generate nonsensical content or fabricate content that might sound reasonable but is inaccurate or outdated. \nLimited Scope for Code: Majority of Phi-3 training data is based in Python and use common packages such as \"typing, math, random, collections, datetime, itertools\". If the model generates Python scripts that utilize other packages or scripts in other languages, we strongly recommend users manually verify all API uses.\nDevelopers should apply responsible AI best practices and are responsible for ensuring that a specific use case complies with relevant laws and regulations (e.g. privacy, trade, etc.). Important areas for consideration include: \nAllocation: Models may not be suitable for scenarios that could have consequential impact on legal status or the allocation of resources or life opportunities (ex: housing, employment, credit, etc.) without further assessments and additional debiasing techniques.\nHigh-Risk Scenarios: Developers should assess suitability of using models in high-risk scenarios where unfair, unreliable or offensive outputs might be extremely costly or lead to harm. This includes providing advice in sensitive or expert domains where accuracy and reliability are critical (ex: legal or health advice). Additional safeguards should be implemented at the application level according to the deployment context.\nMisinformation: Models may produce inaccurate information. Developers should follow transparency best practices and inform end-users they are interacting with an AI system. At the application level, developers can build feedback mechanisms and pipelines to ground responses in use-case specific, contextual information, a technique known as Retrieval Augmented Generation (RAG).\nGeneration of Harmful Content: Developers should assess outputs for their context and use available safety classifiers or custom solutions appropriate for their use case.\nMisuse: Other forms of misuse such as fraud, spam, or malware production may be possible, and developers should ensure that their applications do not violate applicable laws and regulations.\nIdentification of individuals: models with vision capabilities may have the potential to uniquely identify individuals in images. Safety post-training steers the model to refuse such requests, but developers should consider and implement, as appropriate, additional mitigations or user consent flows as required in their respective jurisdiction, (e.g., building measures to blur faces in image inputs before processing.\n\t\tTraining\n\t\tModel\nArchitecture: Phi-3-Vision-128K-Instruct has 4.2B parameters and contains image encoder, connector, projector, and Phi-3 Mini language model.\nInputs: Text and Image. It\u2019s best suited for prompts using the chat format. \nContext length: 128K tokens\nGPUs: 512 H100-80G\nTraining time: 1.5 days\nTraining data: 500B vision and text tokens\nOutputs: Generated text in response to the input\nDates: Our models were trained between February and April 2024\nStatus: This is a static model trained on an offline text dataset with cutoff date Mar 15, 2024. Future versions of the tuned models may be released as we improve models.\nRelease Type: Open weight release\nRelease dates: The model weight is released on May 21, 2024.\n\t\tDatasets\nOur training data includes a wide variety of sources, and is a combination of \npublicly available documents filtered rigorously for quality, selected high-quality educational data and code;\nselected high-quality image-text interleave;\nnewly created synthetic, \u201ctextbook-like\u201d data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (science, daily activities, theory of mind, etc.), newly created image data, e.g., chart/table/diagram/slides;\nhigh quality chat format supervised data covering various topics to reflect human preferences on different aspects such as instruct-following, truthfulness, honesty and helpfulness.\nThe data collection process involved sourcing information from publicly available documents, with a meticulous approach to filtering out undesirable documents and images. To safeguard privacy, we carefully filtered various image and text data sources to remove or scrub any potentially personal data from the training data.\nMore details can be found in the Phi-3 Technical Report.\n\t\tBenchmarks\nTo understand the capabilities, we compare Phi-3-Vision-128K-Instruct with a set of models over a variety of zero-shot benchmarks using our internal benchmark platform.\nBenchmark\nPhi-3 Vision-128K-In\nLlaVA-1.6 Vicuna-7B\nQWEN-VL Chat\nLlama3-Llava-Next-8B\nClaude-3 Haiku\nGemini 1.0 Pro V\nGPT-4V-Turbo\nMMMU\n40.4\n34.2\n39.0\n36.4\n40.7\n42.0\n55.5\nMMBench\n80.5\n76.3\n75.8\n79.4\n62.4\n80.0\n86.1\nScienceQA\n90.8\n70.6\n67.2\n73.7\n72.0\n79.7\n75.7\nMathVista\n44.5\n31.5\n29.4\n34.8\n33.2\n35.0\n47.5\nInterGPS\n38.1\n20.5\n22.3\n24.6\n32.1\n28.6\n41.0\nAI2D\n76.7\n63.1\n59.8\n66.9\n60.3\n62.8\n74.7\nChartQA\n81.4\n55.0\n50.9\n65.8\n59.3\n58.0\n62.3\nTextVQA\n70.9\n64.6\n59.4\n55.7\n62.7\n64.7\n68.1\nPOPE\n85.8\n87.2\n82.6\n87.0\n74.4\n84.2\n83.7\n\t\tSoftware\nPyTorch\nTransformers\nFlash-Attention\n\t\tHardware\nNote that by default, the Phi-3-Vision-128K model uses flash attention, which requires certain types of GPU hardware to run. We have tested on the following GPU types:\nNVIDIA A100\nNVIDIA A6000\nNVIDIA H100\n\t\tLicense\nThe model is licensed under the MIT license.\n\t\tTrademarks\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow\u202fMicrosoft\u2019s Trademark & Brand Guidelines. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party\u2019s policies.\nDownloads last month200,693\nSafetensors\nModel size\n4.15B params\nTensor type\nBF16\n\t\t\t\t\t\t\u00b7\nText Generation\nInference API (serverless) does not yet support model repos that contain custom code.\n\t\t\t\t\t\tSpaces using\n\t\t\t\t\t\tmicrosoft/Phi-3-vision-128k-instruct\n26\n\ud83d\ude3b\nysharma/Microsoft_Phi-3-Vision-128k\ud83d\udd25\nMaziyarPanahi/Phi-3-vision-128k\ud83d\ude3b\nprithivMLmods/PHI3-VISION\ud83d\udcda\naifeifei798/nemotron-4-340b-instruct\ud83c\udfc3\nsimonraj/PersonalTrainer-Arnold\ud83d\udcda\naifeifei798/Mistral-NeMo-12B-Instruct\ud83c\udf10\ndwb2023/hf_extractor\ud83d\ude3b\nawacke1/ImageChatbot-Phi-3-Vision-128k\ud83d\ude3b\nTypeGPT/Microsoft_Phi-3-Vision-128k\ud83d\udcda\naifeifei798/Llama-3-70B-Instruct\ud83d\udcc8\nbaconnier/BANKER_Vision\ud83d\ude80\ndwb2023/model_explorer4\ud83d\udd2c\ud83e\uddeb\ndwb2023/omniscience\ud83d\ude80\ndwb2023/model_explorer2\ud83c\udf16\nSharryOG/AI\ud83d\udd25\nhf1024/Phi-3-vision-128k\ud83d\ude3b\nnikhilalgorithma/vision1\ud83d\ude3b\naggaranu/Microsoft_Phi-3-Vision-128k\ud83d\ude3b\nzohadev/Microsoft-Phi-3-Vision-128k\ud83d\udd25\nShankarm08/phi3vision\n+ 21 Spaces\n+ 6 Spaces\n\t\t\t\t\t\tCollection including\n\t\t\t\t\t\tmicrosoft/Phi-3-vision-128k-instruct\nPhi-3\n\t\t\t\t\tCollection\nPhi-3 family of small language and multi-modal models. Language models are available in short- and long-context lengths.\n\u2022 \n23 items\n\u2022 \nUpdated\n\t\t\t\t\t11 days ago\n\u2022\n\t\t\t\t\t372\nCompany\n\u00a9 Hugging Face\nTOS\nPrivacy\nAbout\nJobs\nWebsite\nModels\nDatasets\nSpaces\nPricing\nDocs\""}
{"agent": "scrape_agent", "event": "agent_step", "time": "2024-07-22 20:52:45.307759+05:30", "tool": "_Exception", "tool_input": "I did it wrong. Invalid Format: I missed the 'Action:' after 'Thought:'. I will do right next, and don't use a tool I have already used.\n\nIf you don't need to use any more tools, you must give your best complete final answer, make sure it satisfy the expect criteria, use the EXACT format below:\n\nThought: I now can give a great answer\nFinal Answer: my best complete final answer to the task.", "log": ""}
{"agent": "scrape_agent", "event": "agent_step", "time": "2024-07-22 20:52:58.983011+05:30", "tool": "Read website content", "tool_input": "{\"website_url\": \"https://www.artificialintelligence-news.com/news/meta-joins-apple-withholding-ai-models-eu-users/\"}", "log": "Thought: I need to scrape the text from the provided URL.\n\nAction: Read website content\nAction Input: {\"website_url\": \"https://www.artificialintelligence-news.com/news/meta-joins-apple-withholding-ai-models-eu-users/\"}"}
{"agent": "scrape_agent", "event": "agent_finish", "time": "2024-07-22 20:53:25.370776+05:30", "tool": "Unknown", "tool_input": "Unknown", "log": "Thought: I now can give a great answer\n\nFinal Answer: The scraped text from the provided URL is:\n\n\"Meta joins Apple in withholding AI models from EU users\nMenu\n ARTICLE\nLOG IN\nNews\nCategories\nApplications\nChatbots\nFace Recognition\nVirtual Assistants\nVoice Recognition\nCompanies\nAmazon\nApple\nGoogle\nMeta (Facebook)\nMicrosoft\nNVIDIA\nDeep & Reinforcement Learning\nEnterprise\nEthics & Society\nIndustries\nEnergy\nEntertainment & Retail\nGaming\nHealthcare\nManufacturing\nLegislation & Government\nMachine Learning\nPrivacy\nResearch\nRobotics\nSecurity\nSurveillance\nEvents\nWebinars & Resources\nWork With Us\nPublish an article or press release\nAdvertise on AI News\nContact Us\nSubscribe / Login\nMore TechForge News\nThe Block\nCloud Tech News\nDeveloper News\nEdge Computing News\nIoT News\nMarketing Tech News\nTelecoms Tech News\nUpcoming Events\nMeta joins Apple in withholding AI models from EU users\nAbout the Author\nBy Ryan Daws |\n July 18, 2024\nhttps://twitter.com/gadget_ry Categories:\n Artificial Intelligence,\n Companies,\n Development,\n Ethics & Society,\n Legislation & Government,\n Meta (Facebook),\n Ryan Daws is a senior editor at TechForge Media with over a decade of experience in crafting compelling narratives and making complex topics accessible. His articles and interviews with industry leaders have earned him recognition as a key influencer by organisations like Onalytica. Under his leadership, publications have been praised by analyst firms such as Forrester for their excellence and performance. Connect with him on X (@gadget_ry) or Mastodon (@gadgetry@techhub.social)\nMeta has announced it will not be launching its upcoming multimodal AI model in the European Union due to regulatory concerns.\nThis decision from Meta comes on the heels of Apple\u2019s similar move to exclude the EU from its Apple Intelligence rollout, signalling a growing trend of tech giants hesitating to introduce advanced AI technologies in the region.\nMeta\u2019s latest multimodal AI model \u2013 capable of handling video, audio, images, and text \u2013 was set to be released under an open license. However, Meta\u2019s decision will prevent European companies from utilising this technology, potentially putting them at a disadvantage in the global AI race.\n\u201cWe will release a multimodal Llama model over the coming months, but not in the EU due to the unpredictable nature of the European regulatory environment,\u201d a Meta spokesperson stated.\nA text-only version of Meta\u2019s Llama 3 model is still expected to launch in the EU.\nMeta\u2019s announcement comes just days after the EU finalised compliance deadlines for its new AI Act. Tech companies operating in the EU will have until August 2026 to comply with rules surrounding copyright, transparency, and specific AI applications like predictive policing.\nThe withholding of these advanced AI models from the EU market creates a challenging situation for companies outside the region. Those hoping to provide products and services utilising these models will be unable to offer them in one of the world\u2019s largest economic markets.\nMeta plans to integrate its multimodal AI models into products like the Meta Ray-Ban smart glasses. The company\u2019s EU exclusion will extend to future multimodal AI model releases as well.\nMeta will *not* release the multimodal versions of its AI products and models in the EU because of an unpredictable regulatory environment. This means that EU users of Ray-Ban Meta won't be able to use the image understanding features.It also means that the EU industry will not\u2026\u2014 Yann LeCun (@ylecun) July 19, 2024\nAs more tech giants potentially follow suit, the EU may face challenges in maintaining its position as a leader in technological innovation while balancing concerns about AI\u2019s societal impacts.\n(Photo by engin akyurt)\nSee also: AI could unleash \u00a3119 billion in UK productivity\nWant to learn more about AI and big data from industry leaders? Check out AI & Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is co-located with other leading events including Intelligent Automation Conference, BlockX, Digital Transformation Week, and Cyber Security & Cloud Expo.\nExplore other upcoming enterprise technology events and webinars powered by TechForge here.\nTags: ai, ai act, apple, artificial intelligence, eu, europe, european union, meta, regulation\nView Comments\nLeave a comment\nLeave a Reply Cancel replyYou must be logged in to post a comment. \nJoin our community\nCreate your free account now to access all our premium content and recieve the latest tech news to your inbox.\nSUBSCRIBE NOW\nLATEST ARTICLES \nHow AI is transforming sports betting for better odds\nArtificial Intelligence\nUK hospitals begin live trial of prostate cancer-detecting AI\nHealthcare\nMistral AI and NVIDIA unveil 12B NeMo model\nArtificial Intelligence\nTSMC forecasts record growth, rejects US joint venture amid AI surge\nArtificial Intelligence\nOver 40% of Japanese firms lack AI adoption plans\nArtificial Intelligence\nOther NewsArtificial Intelligence News\nBlockchain News\nCloud Computing News\nDeveloper News\nIoT News\nTelecoms News\nEdge Computing News\nView all TechForge Publications\nCookie Policy (UK)\nEventsBlockchain & AI Expo\nAI & Big Data Expo\n#DMWF Digital Marketing World Forum\nDeveloper Events\nIoT Events\nMarketing Events\nTelecoms Events\nCategoriesDeep & Reinforcement LearningMachine LearningRoboticsVoice Recognition \nAI News provides artificial intelligence news and jobs, industry analysis and digital media insight around numerous marketing disciplines; mobile strategy, email marketing, SEO, analytics, social media and much more.\nPlease follow this link for our privacy policy.\nCopyright \u00a9 2024 AI News. All Rights\n Reserved.\nLoginUsername(Required) Password(Required) \nRemember Me\nForgot Password \nNot subscribed / a member yet?\nREGISTER HERE TO SUBSCRIBE NOW\nRegister\n\"*\" indicates required fields\nPersonal DetailsFirst Names* Last Name* Interests*AIBig DataBlockchainCloudCyber SecurityData CentresDevOpsDigital TransformationEdge ComputingEnterpriseFutureworksHigh Performance ComputingIoTMarketing TechQuantum ComputingVR/AR/XRCountry*United KingdomUnited StatesAfghanistanAlbaniaAlgeriaAmerican SamoaAndorraAngolaAnguillaAntarcticaAntigua and BarbudaArgentinaArmeniaArubaAustraliaAustriaAzerbaijanBahamasBahrainBangladeshBarbadosBelarusBelgiumBelizeBeninBermudaBhutanBoliviaBonaire, Sint Eustatius and SabaBosnia and HerzegovinaBotswanaBouvet IslandBrazilBritish Indian Ocean TerritoryBrunei DarussalamBulgariaBurkina FasoBurundiCambodiaCameroonCanadaCape VerdeCayman IslandsCentral African RepublicChadChileChinaChristmas IslandCocos IslandsColombiaComorosCongo, Democratic Republic of theCongo, Republic of theCook IslandsCosta RicaCroatiaCubaCura\u00e7aoCyprusCzech RepublicC\u00f4te d'IvoireDenmarkDjiboutiDominicaDominican RepublicEcuadorEgyptEl SalvadorEquatorial GuineaEritreaEstoniaEswatini (Swaziland)EthiopiaFalkland IslandsFaroe IslandsFijiFinlandFranceFrench GuianaFrench PolynesiaFrench Southern TerritoriesGabonGambiaGeorgiaGermanyGhanaGibraltarGreeceGreenlandGrenadaGuadeloupeGuamGuatemalaGuernseyGuineaGuinea-BissauGuyanaHaitiHeard and McDonald IslandsHoly SeeHondurasHong KongHungaryIcelandIndiaIndonesiaIranIraqIrelandIsle of ManIsraelItalyJamaicaJapanJerseyJordanKazakhstanKenyaKiribatiKuwaitKyrgyzstanLao People's Democratic RepublicLatviaLebanonLesothoLiberiaLibyaLiechtensteinLithuaniaLuxembourgMacauMacedoniaMadagascarMalawiMalaysiaMaldivesMaliMaltaMarshall IslandsMartiniqueMauritaniaMauritiusMayotteMexicoMicronesiaMoldovaMonacoMongoliaMontenegroMontserratMoroccoMozambiqueMyanmarNamibiaNauruNepalNetherlandsNew CaledoniaNew ZealandNicaraguaNigerNigeriaNiueNorfolk IslandNorth KoreaNorthern Mariana IslandsNorwayOmanPakistanPalauPalestine, State ofPanamaPapua New GuineaParaguayPeruPhilippinesPitcairnPolandPortugalPuerto RicoQatarRomaniaRussiaRwandaR\u00e9unionSaint Barth\u00e9lemySaint HelenaSaint Kitts and NevisSaint LuciaSaint MartinSaint Pierre and MiquelonSaint Vincent and the GrenadinesSamoaSan MarinoSao Tome and PrincipeSaudi ArabiaSenegalSerbiaSeychellesSierra LeoneSingaporeSint MaartenSlovakiaSloveniaSolomon IslandsSomaliaSouth AfricaSouth GeorgiaSouth KoreaSouth SudanSpainSri LankaSudanSurinameSvalbard and Jan Mayen IslandsSwedenSwitzerlandSyriaTaiwanTajikistanTanzaniaThailandTimor-LesteTogoTokelauTongaTrinidad and TobagoTunisiaTurkeyTurkmenistanTurks and Caicos IslandsTuvaluUS Minor Outlying IslandsUgandaUkraineUnited Arab EmiratesUruguayUzbekistanVanuatuVenezuelaVietnamVirgin Islands, BritishVirgin Islands, U.S.Wallis and FutunaWestern SaharaYemenZambiaZimbabwe\u00c5land IslandsRegion*RegionAsia-PacificCentral & South AmericaEuropeMiddle East & AfricaNorth AmericaCompany DetailsCompany Name* Job Title* Job Type*Job TypeFounder / PartnerCxODirectorVPSenior ManagerManagerDeveloperOtherPhone*Company Type*Company TypeConsultancy / Advisor / ResearchEnterprise / SMEInvestor / VCPress / MediaPublic SectorService ProviderStart-upSystem IntegratorTechnology Solution ProviderIndustry*Select IndustryAutomotive / Transport / LogisticsBuilt Environment (Inc Real Estate, Construction, Facilities and Cities)Business Functions (Inc Marketing, HR)Communications (Inc Telcos, 5G)Consultancy / Advisor / ResearchConsumer Goods / RetailFinancial services (Inc Banking, Insurance)Healthcare (Inc Pharma)IT ServicesManufacturing / Supply ChainMedia / PR (Inc Government, NfP, Education, Defence)Platforms (inc Software, Hardware, Web, Cloud)Public Sector (Inc Government, NfP, Education, Defence)Regulation / Compliance / LawUtilities / EnergyAccount DetailsEmail*\nUsername* This must be a combination of letters and numbersPassword*\nEnter Password\nConfirm Password\nConsent* I agree to the privacy policy.*By submitting this form, I acknowledge that I have read and understood the Privacy Policy and subscribe to receive communications from TechForge Media Ltd.\nView privacy policyCAPTCHASites - hidden - records where signed upAIBlockchainCloudDeveloperEdgeEnterpriseIoTMarketingTelecomsVRStep 1 of 3\nBACK\nNEXT\nEmailThis field is for validation purposes and should be left unchanged.\nAlready a member / subscriber?\nLOGIN HERE\n\u00d7\nManage Cookie Consent\nWe use technologies like cookies to store and/or access device information. We do this to improve browsing experience and to show personalized ads. Consenting to these technologies will allow us to process data such as browsing behavior or unique IDs on this site. Not consenting or withdrawing consent, may adversely affect certain features and functions.\nFunctional\nFunctional\n\t\t\t\t\t\t\t\tAlways active\t\t\t\t\t\t\t\nThe technical storage or access is strictly necessary for the legitimate purpose of enabling the use of a specific service explicitly requested by the subscriber or user, or for the sole purpose of carrying out the transmission of a communication over an electronic communications network.\nPreferences\nPreferences\nThe technical storage or access is necessary for the legitimate purpose of storing preferences that are not requested by the subscriber or user.\nStatistics\nStatistics\nThe technical storage or access that is used exclusively for statistical purposes.\nThe technical storage or access that is used exclusively for anonymous statistical purposes. Without a subpoena, voluntary compliance on the part of your Internet Service Provider, or additional records from a third party, information stored or retrieved for this purpose alone cannot usually be used to identify you.\nMarketing\nMarketing\nThe technical storage or access is required to create user profiles to send advertising, or to track the user on a website or across several websites for similar marketing purposes.\nManage options\nManage services\nManage {vendor_count} vendors\nRead more about these purposes\nAccept\nDeny\nView preferences\nSave preferences\nView preferences\n{title}\n{title}\n{title}\nManage consent\""}
{"agent": "scrape_agent", "event": "agent_step", "time": "2024-07-22 21:10:13.838229+05:30", "tool": "Read website content", "tool_input": "{\"website_url\": \"https://nationaltechnology.co.uk/Meta_Witholds_Advanced_AI_Model_From_EU.php#:~:text=Meta%20withholds%20advanced%20AI%20model%20from%20EU%20amid%20regulatory%20uncertainty,-By%20Jonathan%20Easton&text=Meta%2C%20the%20parent%20company%20of,an%20%22unpredictable%22%20regulatory%20environment.\"}", "log": "Thought: I need to scrape the content from the provided URL to extract the text data.\n\nAction: Read website content\nAction Input: {\"website_url\": \"https://nationaltechnology.co.uk/Meta_Witholds_Advanced_AI_Model_From_EU.php#:~:text=Meta%20withholds%20advanced%20AI%20model%20from%20EU%20amid%20regulatory%20uncertainty,-By%20Jonathan%20Easton&text=Meta%2C%20the%20parent%20company%20of,an%20%22unpredictable%22%20regulatory%20environment.\"}"}
{"agent": "scrape_agent", "event": "agent_finish", "time": "2024-07-22 21:10:19.257695+05:30", "tool": "Unknown", "tool_input": "Unknown", "log": "Thought: I now know the final answer\n\nFinal Answer: \nHere is the scraped text from the provided URL:\n\n\"Meta withholds advanced AI model from EU amid regulatory uncertainty - National Technology\nEstablished 2016\nHome\nAbout \nSign Up \nContact\nEvents\nVideos\nRoundtables\nPodcasts\nNews Archive\nFeatures\nWhitepapers\nDigital Archive\nHome\nAbout \nSign Up \nContact\nEvents\nVideos\nRoundtables\nPodcasts\nNews Archive\nFeatures\nWhitepapers\nDigital Archive\nNational Technology Awards\nRetail Systems \nFStech\nLatest News\n Oracle agrees to $115m settlement in privacy lawsuit\n UK government invests over \u00a3100m in green aerospace tech\n WazirX pauses trading after $230m cyber-attack loss\n Ocado to supply automation across Kroger's warehouse network\n Microsoft global IT outage impacts banks, airlines and businesses\n Amazon tackles fake reviews with joint lawsuit\n\t\t\t\t\t\tMeta withholds advanced AI model from EU amid regulatory uncertainty\n By Jonathan Easton\n 19/07/2024\nMeta, the parent company of Facebook and Instagram, has announced it will not release its upcoming multimodal artificial intelligence (AI) model in the European Union due to what it calls an \"unpredictable\" regulatory environment.\nThe US tech giant plans to launch the advanced AI model, capable of processing text, images, audio, and video, in the coming months. However, European customers and companies will be unable to access or utilise the technology.\n\"We will release a multimodal Llama model over the coming months, but not in the EU due to the unpredictable nature of the European regulatory environment,\" a Meta spokesperson told Axios.\nThe decision highlights growing tensions between American tech firms and EU regulators. Last month, Apple made a similar move, withholding its Apple Intelligence features from Europe over regulatory concerns.\nMeta's primary issue stems from difficulties in training AI models using European user data while complying with the General Data Protection Regulation (GDPR). In June, the company was ordered to pause AI training on EU data after announcing plans to use public posts from Facebook and Instagram users.\nThe Irish Data Protection Commission, Meta's lead privacy regulator in Europe, said discussions with the company over model training were ongoing.\nDespite nearly identical data protection laws, Meta plans to launch the new model for UK users, citing less regulatory uncertainty in Britain.\nThe withholding of the multimodal AI model could have far-reaching implications for European businesses and consumers. Companies outside the EU may be prevented from offering products and services in Europe that utilise the new technology.\nMeta argues that training on European data is crucial for ensuring its products accurately reflect regional terminology and culture. The company claims competitors such as Google and OpenAI are already training on European data.\nWhile the multimodal model will be withheld, Meta still plans to release a larger, text-only version of its Llama 3 model in the EU.\nThe move underscores the ongoing struggle between innovation and regulation in the rapidly evolving field of AI, with European users potentially missing out on cutting-edge technologies as a result.\nShare Story:\nRecent Stories\nSamsung to acquire British AI startup Oxford Semantic\n READ STORY \nMeta withholds advanced AI model from EU amid regulatory uncertainty\n READ STORY \nAccenture acquires Logic to bolster retail tech arm\n READ STORY \nBringing Teams to the table \u2013 Adding value by integrating Microsoft Teams with business applicationsYour browser does not support HTML5 video.A decade ago, the idea of digital collaboration started and ended with sending documents over email. Some organisations would have portals for sharing content or simplistic IM apps, but the ways that we communicated online were still largely primitive.\nPopular\nRecent\n1\n Microsoft global IT outage impacts banks, airlines and businesses \n2\n Carrefour to roll out solar power across 350 French stores \n3\n Oracle agrees to $115m settlement in privacy lawsuit\n4\n Samsung to acquire British AI startup Oxford Semantic\n5\n UK financial authorities to examine 'seismic shift' in digital payments\n6\n Tech industry responds to \u2018missing\u2019 AI Bill in King\u2019s Speech\n7\n Meta withholds advanced AI model from EU amid regulatory uncertainty\n8\n Starling founder stepping down from board to 'create AI company'\n9\n TikTok owner loses appeal over Digital Markets Act \n10\n FTC seeks details on Amazon's deal with AI startup Adept\n1\nUK government invests over \u00a3100m in green aerospace tech \n2\nOcado to supply automation across Kroger's warehouse network\n3\nWazirX pauses trading after $230m cyber-attack loss \n4\nOracle agrees to $115m settlement in privacy lawsuit\n5\nUSPS \u2018shared customer postal addresses\u2019 with Meta, LinkedIn and Snap \n6\nAmazon tackles fake reviews with joint lawsuit\n7\nMicrosoft global IT outage impacts banks, airlines and businesses \n8\nMeta withholds advanced AI model from EU amid regulatory uncertainty\n9\nMeta suspends use of gen AI tools in Brazil \n10\nAustralian police union calls for \u2018one-stop portal\u2019 for victims of AI deepfakes\nAutomating CX: How are businesses using AI to meet customer expectations? Your browser does not support HTML5 video.Virtual agents are set to supplant the traditional chatbot and their use cases are evolving at pace, with many organisations deploying new AI technologies to meet rising customer demand for self-service and real-time interactions.\nExpert Views\n1\nThe Sustainable Data Center: Getting More Done, and Leaving Less Behind\n2\nGet ahead in the cloud with Daisy: 7 benefits of Microsoft Azure\n3\nBright Data acquires Market Beyond to add digital shelf analytics to its data offerings\n4\nResearch reveals use of web content tools is expected to grow as internet restrictions continue to tighten\n5\nInfluencing the social web space with public data\n\t\tWe use cookies to ensure you get the best experience on our website. Learn More\n\t\t\tAccept\n \u00a9 2019 Perspective Publishing Privacy & Cookies\""}
